{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 11:54:10.477759: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-10 11:54:10.762398: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-10 11:54:10.763999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-10 11:54:11.786527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/johtorr/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import sys\n",
    "sys.path.append('/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/')\n",
    "from models.ts_model import TS_Data, TSModel\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Run on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trace():\n",
    "    df = pd.read_csv(\"../data/traffic/Metro_Interstate_Traffic_Volume.csv\")\n",
    "    holiday = (df[\"holiday\"].values == None).astype(np.float32)\n",
    "    temp = df[\"temp\"].values.astype(np.float32)\n",
    "    temp -= np.mean(temp)  # normalize temp by annual mean\n",
    "    rain = df[\"rain_1h\"].values.astype(np.float32)\n",
    "    snow = df[\"snow_1h\"].values.astype(np.float32)\n",
    "    clouds = df[\"clouds_all\"].values.astype(np.float32)\n",
    "    date_time = df[\"date_time\"].values\n",
    "    # 2012-10-02 13:00:00\n",
    "    date_time = [dt.datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\") for d in date_time]\n",
    "    weekday = np.array([d.weekday() for d in date_time]).astype(np.float32)\n",
    "    noon = np.array([d.hour for d in date_time]).astype(np.float32)\n",
    "    noon = np.sin(noon * np.pi / 24)\n",
    "\n",
    "    features = np.stack([holiday, temp, rain, snow, clouds, weekday, noon], axis=-1)\n",
    "\n",
    "    traffic_volume = df[\"traffic_volume\"].values.astype(np.float32)\n",
    "    traffic_volume -= np.mean(traffic_volume)  # normalize\n",
    "    traffic_volume /= np.std(traffic_volume)  # normalize\n",
    "\n",
    "    return features, traffic_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48204, 7), (48204,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ltc'\n",
    "model_size = 32\n",
    "epochs = 2\n",
    "seq_len = 6\n",
    "log = -1\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training sequences: 12050\n"
     ]
    }
   ],
   "source": [
    "input_ts_data = TS_Data(x = x, y = y, seq_len = seq_len, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/models/ts_model.py:89: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/johtorr/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/keras/src/initializers/initializers_v1.py:431: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "logit shape:  (?, ?, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 11:54:28.785462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-09-10 11:54:28.807068: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 000, train loss: 0.27, train mae: 0.40, valid loss: 1.00, valid mae: 0.88, test loss: 1.00, test mae: 0.87\n",
      "Epochs 001, train loss: 0.23, train mae: 0.37, valid loss: 0.19, valid mae: 0.33, test loss: 0.18, test mae: 0.32\n",
      "INFO:tensorflow:Restoring parameters from tf_sessions/traffic/ltc\n",
      "Best epoch 001, train loss: 0.267, train mae: 0.404, valid loss: 0.189, valid mae: 0.325, test loss: 0.176, test mae: 0.321\n"
     ]
    }
   ],
   "source": [
    "model = TSModel(model_type=model_type, model_size=model_size)\n",
    "model.fit(input_ts_data, epochs=epochs, log_period=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.        ,  -1.4758606 ,   0.        , ...,  12.        ,\n",
       "           4.        ,   0.86602545],\n",
       "        [  0.        ,  12.494141  ,   4.05      , ...,  92.        ,\n",
       "           1.        ,   0.70710677],\n",
       "        [  0.        ,  10.494141  ,   0.        , ...,   5.        ,\n",
       "           5.        ,   0.9914449 ],\n",
       "        ...,\n",
       "        [  0.        ,  -7.625885  ,   0.        , ...,  90.        ,\n",
       "           4.        ,   0.49999985],\n",
       "        [  0.        ,  -7.0558777 ,   0.        , ...,  90.        ,\n",
       "           1.        ,   0.8660254 ],\n",
       "        [  0.        , -15.055878  ,   0.        , ...,  90.        ,\n",
       "           0.        ,   0.70710677]],\n",
       "\n",
       "       [[  0.        ,  -0.4058838 ,   0.        , ...,  90.        ,\n",
       "           4.        ,   0.9238795 ],\n",
       "        [  0.        ,  11.75412   ,   3.8       , ...,  92.        ,\n",
       "           1.        ,   0.6087615 ],\n",
       "        [  0.        ,  12.134125  ,   0.        , ...,   5.        ,\n",
       "           5.        ,   1.        ],\n",
       "        ...,\n",
       "        [  0.        ,  -7.625885  ,   0.        , ...,  90.        ,\n",
       "           4.        ,   0.49999985],\n",
       "        [  0.        ,  -7.0558777 ,   0.        , ...,  90.        ,\n",
       "           1.        ,   0.8660254 ],\n",
       "        [  0.        , -15.125885  ,   0.        , ...,  90.        ,\n",
       "           0.        ,   0.7933534 ]],\n",
       "\n",
       "       [[  0.        ,   0.8441162 ,   0.        , ...,  12.        ,\n",
       "           4.        ,   0.9659259 ],\n",
       "        [  0.        ,  11.444122  ,   1.11      , ...,  92.        ,\n",
       "           1.        ,   0.49999985],\n",
       "        [  0.        ,  13.324127  ,   0.        , ...,   5.        ,\n",
       "           5.        ,   0.9914449 ],\n",
       "        ...,\n",
       "        [  0.        ,  -7.9658813 ,   0.        , ...,  90.        ,\n",
       "           4.        ,   0.38268328],\n",
       "        [  0.        ,  -7.0558777 ,   0.        , ...,  90.        ,\n",
       "           1.        ,   0.8660254 ],\n",
       "        [  0.        , -14.345886  ,   0.        , ...,  90.        ,\n",
       "           0.        ,   0.86602545]],\n",
       "\n",
       "       [[  0.        ,   0.9441223 ,   0.        , ...,  40.        ,\n",
       "           4.        ,   0.9914449 ],\n",
       "        [  0.        ,  10.974121  ,   0.        , ...,  92.        ,\n",
       "           1.        ,   0.38268328],\n",
       "        [  0.        ,  13.964142  ,   0.        , ...,   1.        ,\n",
       "           5.        ,   0.9659258 ],\n",
       "        ...,\n",
       "        [  0.        ,  -7.9658813 ,   0.        , ...,  90.        ,\n",
       "           4.        ,   0.38268328],\n",
       "        [  0.        ,  -7.0558777 ,   0.        , ...,  90.        ,\n",
       "           1.        ,   0.8660254 ],\n",
       "        [  0.        , -14.125885  ,   0.        , ...,  90.        ,\n",
       "           0.        ,   0.9238795 ]],\n",
       "\n",
       "       [[  0.        ,   1.9941406 ,   0.        , ...,  48.        ,\n",
       "           4.        ,   1.        ],\n",
       "        [  0.        ,   9.51413   ,   0.        , ...,  92.        ,\n",
       "           1.        ,   0.2588189 ],\n",
       "        [  0.        ,  14.554138  ,   0.        , ...,   5.        ,\n",
       "           5.        ,   0.92387956],\n",
       "        ...,\n",
       "        [  0.        ,  -8.305878  ,   0.        , ...,  90.        ,\n",
       "           4.        ,   0.2588189 ],\n",
       "        [  0.        ,  -7.0558777 ,   0.        , ...,  90.        ,\n",
       "           1.        ,   0.7933533 ],\n",
       "        [  0.        , -12.195862  ,   0.        , ...,  75.        ,\n",
       "           0.        ,   0.9659259 ]],\n",
       "\n",
       "       [[  0.        ,   2.334137  ,   0.        , ...,  48.        ,\n",
       "           4.        ,   0.9914449 ],\n",
       "        [  0.        ,   9.124115  ,   0.        , ...,  92.        ,\n",
       "           1.        ,   0.13052632],\n",
       "        [  0.        ,  14.834137  ,   0.        , ...,   5.        ,\n",
       "           5.        ,   0.8660254 ],\n",
       "        ...,\n",
       "        [  0.        ,  -8.305878  ,   0.        , ...,  90.        ,\n",
       "           4.        ,   0.2588189 ],\n",
       "        [  0.        ,  -7.0558777 ,   0.        , ...,  90.        ,\n",
       "           1.        ,   0.7933533 ],\n",
       "        [  0.        , -12.195862  ,   0.        , ...,  75.        ,\n",
       "           0.        ,   0.9659259 ]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iterate_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mpredict(input_ts_data\u001b[39m.\u001b[39;49mtest_x)\n",
      "File \u001b[0;32m/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/models/ts_model.py:246\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(self, gesture_data)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult_file, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    234\u001b[0m         f\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m    235\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m{:08d}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{:0.8f}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{:0.8f}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{:0.8f}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{:0.8f}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{:0.8f}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{:0.8f}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    236\u001b[0m                 best_epoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m             )\n\u001b[1;32m    244\u001b[0m         )\n\u001b[0;32m--> 246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, gesture_data):\n\u001b[1;32m    247\u001b[0m     losses \u001b[39m=\u001b[39m []\n\u001b[1;32m    248\u001b[0m     accs \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iterate_train'"
     ]
    }
   ],
   "source": [
    "model.predict(input_ts_data.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.333480858597035"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48204/9038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_x = []\n",
    "sequences_y = []\n",
    "\n",
    "for s in range(0, x.shape[0] - seq_len, 4):\n",
    "    start = s\n",
    "    end = start + seq_len\n",
    "    #print(\"start:\", s, \"end:\", end)\n",
    "    sequences_x.append(x[start:end])\n",
    "    sequences_y.append(y[start:end])\n",
    "\n",
    "x_input_seq = np.stack(sequences_x, axis=1)\n",
    "y_input_seq = np.stack(sequences_y, axis=1)\n",
    "total_seqs = x_input_seq.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12050"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.RandomState(123).permutation(total_seqs)\n",
    "valid_size = int(0.1 * total_seqs)\n",
    "test_size = int(0.15 * total_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1205"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = x_input_seq[:, permutation[:valid_size]]\n",
    "valid_y = y_input_seq[:, permutation[:valid_size]]\n",
    "test_x = x_input_seq[:, permutation[valid_size : valid_size + test_size]]\n",
    "test_y = y_input_seq[:, permutation[valid_size : valid_size + test_size]]\n",
    "x_input_seq = x_input_seq[:, permutation[valid_size + test_size :]]\n",
    "y_input_seq = y_input_seq[:, permutation[valid_size + test_size :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 1205, 7), (6, 1205))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 1807, 7), (6, 1807))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 9038, 7), (6, 9038))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input_seq.shape, y_input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "total_seqs = x_input_seq.shape[1]\n",
    "permutation = np.random.permutation(total_seqs)\n",
    "total_batches = total_seqs // batch_size\n",
    "\n",
    "for i in range(total_batches):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    batch_x = x_input_seq[:, permutation[start:end]]\n",
    "    batch_y = y_input_seq[:, permutation[start:end]]\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5507145]], dtype=float32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y[0][:1].reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 7)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 16, 7)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5507145 ,  1.2518278 , -1.460016  , -1.0880686 , -0.13227957,\n",
       "         0.32221097,  0.63778746,  0.51548266,  0.61161524, -0.8847307 ,\n",
       "        -0.06131261, -0.23797505,  0.5285688 , -0.29434597, -0.91945916,\n",
       "        -1.4736054 ]], dtype=float32)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.0000000e+00,  4.3041382e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          1.0000000e+00,  2.0000000e+00,  9.6592587e-01],\n",
       "        [ 0.0000000e+00,  5.6041260e+00,  7.5999999e-01,  0.0000000e+00,\n",
       "          5.6000000e+01,  0.0000000e+00,  8.6602545e-01],\n",
       "        [ 0.0000000e+00,  6.8141174e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          1.0000000e+00,  3.0000000e+00,  3.8268346e-01],\n",
       "        [ 0.0000000e+00, -5.1458740e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          9.0000000e+01,  6.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  1.5354126e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "          7.5000000e+01,  2.0000000e+00,  3.8268328e-01],\n",
       "        [ 0.0000000e+00, -6.8558655e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          9.0000000e+01,  4.0000000e+00,  7.9335332e-01],\n",
       "        [ 0.0000000e+00, -5.3758850e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          4.0000000e+01,  6.0000000e+00,  9.6592581e-01],\n",
       "        [ 0.0000000e+00, -1.9405884e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "          1.0000000e+00,  1.0000000e+00,  7.0710677e-01],\n",
       "        [ 0.0000000e+00,  1.3074127e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "          4.0000000e+01,  1.0000000e+00,  9.9144489e-01],\n",
       "        [ 0.0000000e+00, -1.7425873e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "          4.0000000e+01,  1.0000000e+00,  1.3052620e-01],\n",
       "        [ 0.0000000e+00, -8.7258606e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          9.0000000e+01,  4.0000000e+00,  3.8268328e-01],\n",
       "        [ 0.0000000e+00, -1.5869141e-02,  0.0000000e+00,  0.0000000e+00,\n",
       "          1.0000000e+02,  5.0000000e+00,  8.6602545e-01],\n",
       "        [ 0.0000000e+00,  1.9274139e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "          1.0000000e+00,  5.0000000e+00,  1.0000000e+00],\n",
       "        [ 0.0000000e+00,  1.1154114e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "          1.0000000e+00,  5.0000000e+00,  3.8268328e-01],\n",
       "        [ 0.0000000e+00, -2.0545868e+01,  0.0000000e+00,  0.0000000e+00,\n",
       "          2.0000000e+01,  1.0000000e+00,  2.5881889e-01],\n",
       "        [ 0.0000000e+00,  9.3241272e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          9.0000000e+01,  1.0000000e+00,  1.3052620e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y prediction  [[[ 0.7842461 ]\n",
      "  [ 1.1884642 ]\n",
      "  [-0.9494908 ]\n",
      "  [-1.2282209 ]\n",
      "  [-0.9567351 ]\n",
      "  [ 0.975094  ]\n",
      "  [-0.20665249]\n",
      "  [ 0.65338534]\n",
      "  [ 0.669698  ]\n",
      "  [-1.391655  ]\n",
      "  [-1.0266671 ]\n",
      "  [ 0.3377916 ]\n",
      "  [ 0.23430961]\n",
      "  [-1.0635269 ]\n",
      "  [-1.3088336 ]\n",
      "  [-1.3294183 ]]]\n"
     ]
    }
   ],
   "source": [
    "_, _, y_hat = model.sess.run(\n",
    "    [model.accuracy, model.loss, model.y],\n",
    "    {model.x: batch_x[:1], model.target_y: batch_y[:1]},\n",
    ")\n",
    "\n",
    "print(\"y prediction \", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m _, _, y_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39msess\u001b[39m.\u001b[39mrun(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     [],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     {model\u001b[39m.\u001b[39mx: batch_x[:\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mtarget_y: batch_y[:\u001b[39m1\u001b[39m]},\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39my prediction \u001b[39m\u001b[39m\"\u001b[39m, y_hat)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"
     ]
    }
   ],
   "source": [
    "_, _, y_hat = model.sess.run(\n",
    "    [],\n",
    "    {model.x: batch_x[:1], model.target_y: batch_y[:1]},\n",
    ")\n",
    "\n",
    "print(\"y prediction \", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument `fetch` = [[ 0.0000000e+00  1.5254120e+01  0.0000000e+00  0.0000000e+00\n   1.2000000e+01  1.0000000e+00  7.0710677e-01]\n [ 0.0000000e+00  1.0874115e+01  0.0000000e+00  0.0000000e+00\n   5.0000000e+00  5.0000000e+00  9.9144489e-01]\n [ 0.0000000e+00 -7.6258850e+00  0.0000000e+00  0.0000000e+00\n   9.0000000e+01  4.0000000e+00  6.0876149e-01]\n [ 0.0000000e+00  5.2741394e+00  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  2.0000000e+00  2.5881904e-01]\n [ 0.0000000e+00 -2.2858582e+00  0.0000000e+00  0.0000000e+00\n   6.4000000e+01  4.0000000e+00  1.3052632e-01]\n [ 0.0000000e+00 -1.1045868e+01  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  0.0000000e+00  7.9335338e-01]\n [ 0.0000000e+00 -3.1125870e+01  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  2.0000000e+00  5.0000000e-01]\n [ 0.0000000e+00  2.3941345e+00  0.0000000e+00  0.0000000e+00\n   2.0000000e+01  6.0000000e+00  7.9335338e-01]\n [ 0.0000000e+00  1.8834137e+01  0.0000000e+00  0.0000000e+00\n   7.5000000e+01  6.0000000e+00  9.9144489e-01]\n [ 0.0000000e+00  8.3441162e+00  0.0000000e+00  0.0000000e+00\n   9.0000000e+01  6.0000000e+00  2.5881904e-01]\n [ 0.0000000e+00 -2.2175873e+01  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  5.0000000e+00  7.9335338e-01]\n [ 0.0000000e+00  5.6641235e+00  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  2.0000000e+00  1.3052620e-01]\n [ 0.0000000e+00  2.5841370e+00  0.0000000e+00  0.0000000e+00\n   5.0000000e+00  6.0000000e+00  2.5881889e-01]\n [ 0.0000000e+00  8.2141418e+00  0.0000000e+00  0.0000000e+00\n   4.0000000e+01  3.0000000e+00  1.3052632e-01]\n [ 0.0000000e+00 -6.5856934e-02  0.0000000e+00  0.0000000e+00\n   7.5000000e+01  1.0000000e+00  1.3052620e-01]\n [ 0.0000000e+00  3.8741150e+00  0.0000000e+00  0.0000000e+00\n   4.0000000e+01  6.0000000e+00  7.0710677e-01]] has invalid type \"ndarray\" must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:305\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39;49mget_default_graph()\u001b[39m.\u001b[39;49mas_graph_element(\n\u001b[1;32m    306\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, allow_operation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    307\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3580\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 3580\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3676\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3674\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3675\u001b[0m   \u001b[39m# We give up!\u001b[39;00m\n\u001b[0;32m-> 3676\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not convert a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m into a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   3677\u001b[0m                   (\u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, types_str))\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_x, batch_y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_x, batch_y):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49msess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         {model\u001b[39m.\u001b[39;49mx: batch_x, model\u001b[39m.\u001b[39;49mtarget_y: batch_y},\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mconstrain_op \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Forecasting/liquid_time_constant_networks/experiments_with_ltcs/notebooks/traffic.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         model\u001b[39m.\u001b[39msess\u001b[39m.\u001b[39mrun(model\u001b[39m.\u001b[39mconstrain_op)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:969\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    970\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    971\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    972\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:1177\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       feed_map[compat\u001b[39m.\u001b[39mas_bytes(subfeed_t\u001b[39m.\u001b[39mname)] \u001b[39m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[1;32m   1176\u001b[0m \u001b[39m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[0;32m-> 1177\u001b[0m fetch_handler \u001b[39m=\u001b[39m _FetchHandler(\n\u001b[1;32m   1178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, fetches, feed_dict_tensor, feed_handles\u001b[39m=\u001b[39;49mfeed_handles)\n\u001b[1;32m   1180\u001b[0m \u001b[39m# Run request and get response.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[39m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:486\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \n\u001b[1;32m    476\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39m    direct feeds.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m--> 486\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_mapper \u001b[39m=\u001b[39m _FetchMapper\u001b[39m.\u001b[39;49mfor_fetch(fetches)\n\u001b[1;32m    487\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches \u001b[39m=\u001b[39m []\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_targets \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:269\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[39mreturn\u001b[39;00m _ListFetchMapper(fetch)\n\u001b[1;32m    268\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, collections_abc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m--> 269\u001b[0m   \u001b[39mreturn\u001b[39;00m _DictFetchMapper(fetch)\n\u001b[1;32m    270\u001b[0m \u001b[39melif\u001b[39;00m _is_attrs_instance(fetch):\n\u001b[1;32m    271\u001b[0m   \u001b[39mreturn\u001b[39;00m _AttrsFetchMapper(fetch)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:417\u001b[0m, in \u001b[0;36m_DictFetchMapper.__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    414\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type_ctor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_type\n\u001b[1;32m    416\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys \u001b[39m=\u001b[39m fetches\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m--> 417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers \u001b[39m=\u001b[39m [\n\u001b[1;32m    418\u001b[0m     _FetchMapper\u001b[39m.\u001b[39mfor_fetch(fetch) \u001b[39mfor\u001b[39;00m fetch \u001b[39min\u001b[39;00m fetches\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    419\u001b[0m ]\n\u001b[1;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_indices \u001b[39m=\u001b[39m _uniquify_fetches(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:418\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type_ctor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetch_type\n\u001b[1;32m    416\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys \u001b[39m=\u001b[39m fetches\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m    417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 418\u001b[0m     _FetchMapper\u001b[39m.\u001b[39;49mfor_fetch(fetch) \u001b[39mfor\u001b[39;00m fetch \u001b[39min\u001b[39;00m fetches\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    419\u001b[0m ]\n\u001b[1;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_indices \u001b[39m=\u001b[39m _uniquify_fetches(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mappers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:277\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fetch, tensor_type):\n\u001b[1;32m    276\u001b[0m       fetches, contraction_fn \u001b[39m=\u001b[39m fetch_fn(fetch)\n\u001b[0;32m--> 277\u001b[0m       \u001b[39mreturn\u001b[39;00m _ElementFetchMapper(fetches, contraction_fn)\n\u001b[1;32m    278\u001b[0m \u001b[39m# Did not find anything.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    280\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/client/session.py:308\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    305\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_fetches\u001b[39m.\u001b[39mappend(ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39mas_graph_element(\n\u001b[1;32m    306\u001b[0m       fetch, allow_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_operation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    307\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 308\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m has invalid type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    309\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(fetch)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m must be a string or Tensor. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    310\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    312\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mArgument `fetch` = \u001b[39m\u001b[39m{\u001b[39;00mfetch\u001b[39m}\u001b[39;00m\u001b[39m cannot be interpreted as \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    313\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma Tensor. (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `fetch` = [[ 0.0000000e+00  1.5254120e+01  0.0000000e+00  0.0000000e+00\n   1.2000000e+01  1.0000000e+00  7.0710677e-01]\n [ 0.0000000e+00  1.0874115e+01  0.0000000e+00  0.0000000e+00\n   5.0000000e+00  5.0000000e+00  9.9144489e-01]\n [ 0.0000000e+00 -7.6258850e+00  0.0000000e+00  0.0000000e+00\n   9.0000000e+01  4.0000000e+00  6.0876149e-01]\n [ 0.0000000e+00  5.2741394e+00  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  2.0000000e+00  2.5881904e-01]\n [ 0.0000000e+00 -2.2858582e+00  0.0000000e+00  0.0000000e+00\n   6.4000000e+01  4.0000000e+00  1.3052632e-01]\n [ 0.0000000e+00 -1.1045868e+01  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  0.0000000e+00  7.9335338e-01]\n [ 0.0000000e+00 -3.1125870e+01  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  2.0000000e+00  5.0000000e-01]\n [ 0.0000000e+00  2.3941345e+00  0.0000000e+00  0.0000000e+00\n   2.0000000e+01  6.0000000e+00  7.9335338e-01]\n [ 0.0000000e+00  1.8834137e+01  0.0000000e+00  0.0000000e+00\n   7.5000000e+01  6.0000000e+00  9.9144489e-01]\n [ 0.0000000e+00  8.3441162e+00  0.0000000e+00  0.0000000e+00\n   9.0000000e+01  6.0000000e+00  2.5881904e-01]\n [ 0.0000000e+00 -2.2175873e+01  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  5.0000000e+00  7.9335338e-01]\n [ 0.0000000e+00  5.6641235e+00  0.0000000e+00  0.0000000e+00\n   1.0000000e+00  2.0000000e+00  1.3052620e-01]\n [ 0.0000000e+00  2.5841370e+00  0.0000000e+00  0.0000000e+00\n   5.0000000e+00  6.0000000e+00  2.5881889e-01]\n [ 0.0000000e+00  8.2141418e+00  0.0000000e+00  0.0000000e+00\n   4.0000000e+01  3.0000000e+00  1.3052632e-01]\n [ 0.0000000e+00 -6.5856934e-02  0.0000000e+00  0.0000000e+00\n   7.5000000e+01  1.0000000e+00  1.3052620e-01]\n [ 0.0000000e+00  3.8741150e+00  0.0000000e+00  0.0000000e+00\n   4.0000000e+01  6.0000000e+00  7.0710677e-01]] has invalid type \"ndarray\" must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in zip(batch_x[0][1].reshape(1,1,7), batch_y[0][:1].reshape(1,1)):\n",
    "    y_hat = model.sess.run(\n",
    "        {model.x: batch_x, model.target_y: batch_y},\n",
    "    )\n",
    "    if not model.constrain_op is None:\n",
    "        model.sess.run(model.constrain_op)\n",
    "    \n",
    "    print(\"Input data \", batch_x)\n",
    "    print(\"y prediction \", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
